{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "from data_reader_new import *\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import fftpack\n",
    "# import h5py \n",
    "import random\n",
    "import shutil\n",
    "from scipy.signal import iirnotch, lfilter, filtfilt\n",
    "from scipy.fft import rfft, rfftfreq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants and Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Constants\n",
    "# Define bandpass filter constants\n",
    "lowcut = 0.5\n",
    "highcut = 100\n",
    "fs = 1024\n",
    "resampleFS = 250\n",
    "\n",
    "# Create the notch filter\n",
    "notch_1_b, notch_1_a = iirnotch(1, Q=30, fs=resampleFS)\n",
    "notch_60_b, notch_60_a = iirnotch(60, Q=30, fs=resampleFS)\n",
    "# notch_100_b, notch_100_a = iirnotch(100, Q=30, fs=250)\n",
    "\n",
    "# Define segment interval length in sec\n",
    "segment_interval = 4\n",
    "print('Segment Interval: ', segment_interval)\n",
    "\n",
    "# Seizure types\n",
    "\n",
    "binary_classifier_flag = True\n",
    "\n",
    "# seizure_types = ['fnsz', 'gnsz', 'cpsz', 'absz', 'tnsz', 'tcsz', 'bckg']\n",
    "if binary_classifier_flag:\n",
    "    seizure_types = ['bckg', 'seizure']\n",
    "    seizure_session_downsampling_ratio = [1, 1]\n",
    "    seizure_overlapping_ratio = [0, 0.75]\n",
    "else:\n",
    "    seizure_types = ['fnsz', 'gnsz', 'cpsz', 'bckg']\n",
    "    seizure_session_downsampling_ratio = [1, 1, 1, 1]\n",
    "    seizure_overlapping_ratio = [0.75, 0.75, 0.75, 0]\n",
    "\n",
    "########## Data path\n",
    "\n",
    "train_val_root = os.path.join('/datadrive', 'TUSZ', 'edf', 'train', '01_tcp_ar')\n",
    "test_root = os.path.join('/datadrive', 'TUSZ', 'edf', 'dev', '01_tcp_ar')\n",
    "# test_root = os.path.join('/datadrive', 'TUSZ', 'edf', 'eval', '01_tcp_ar')\n",
    "\n",
    "\n",
    "if binary_classifier_flag:\n",
    "    save_root = os.path.join('/datadrive', 'TUSZ', 'new_3_TUSZ_processed_binary_individual_segments')\n",
    "else:\n",
    "    save_root = os.path.join('/datadrive', 'TUSZ', 'new_3_TUSZ_processed_multiclass_individual_segments')\n",
    "\n",
    "print(save_root)\n",
    "\n",
    "if not os.path.exists(save_root):\n",
    "    os.mkdir(save_root)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To delete previous data repo when running a new experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To delete previous data repo when running a new experiment\n",
    "## This is to avoid concatenating duplicate data to the existing .npy files\n",
    "if not os.path.exists(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec')):\n",
    "    os.mkdir(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec'))\n",
    "else:\n",
    "    for filename in os.listdir(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec')):\n",
    "        file_path = os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec', filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_paths, train_val_patients = get_all_TUH_EEG_session_paths(train_val_root)\n",
    "print('Total sessions: ', len(train_val_paths))\n",
    "print('Train and val patients: ', len(train_val_patients))\n",
    "\n",
    "test_paths, test_patients = get_all_TUH_EEG_session_paths(test_root)\n",
    "print('Test sessions: ', len(test_paths))\n",
    "print('Test patients: ', len(test_patients))\n",
    "\n",
    "all_paths = train_val_paths + test_paths\n",
    "print('All sessions: ', len(all_paths))\n",
    "\n",
    "training_data = [[] for i in range(len(seizure_types))]\n",
    "validation_data = [[] for i in range(len(seizure_types))]\n",
    "testing_data = [[] for i in range(len(seizure_types))]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_val_patients)\n",
    "train_patients = train_val_patients[:int(len(train_val_patients)*0.8)]\n",
    "val_patients = train_val_patients[int(len(train_val_patients)*0.8):]\n",
    "print('Train patients: ', len(train_patients))\n",
    "print('Val patients: ', len(val_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_paths in [train_val_paths[:], test_paths[:]]:\n",
    "\n",
    "    count_session = 0\n",
    "\n",
    "    for data_path in session_paths:\n",
    "\n",
    "        patient = data_path.split('01_tcp_ar')[1][5:13]\n",
    "        patient_session_token = patient + '_' + data_path.split('01_tcp_ar')[1][14:18] + '_' + data_path[-8:-4]\n",
    "        if patient in train_patients:\n",
    "            # continue\n",
    "            flag_train_val_test = 'train'\n",
    "        elif patient in val_patients:\n",
    "            # continue\n",
    "            flag_train_val_test = 'val'\n",
    "        elif patient in test_patients:\n",
    "            flag_train_val_test = 'test'\n",
    "\n",
    "        count_session += 1\n",
    "        \n",
    "        ### Read the raw signals\n",
    "        raw = mne.io.read_raw_edf(data_path, verbose='warning', preload=True)\n",
    "        # print(raw.info)    \n",
    "        thisFS = int(raw.info['sfreq'])\n",
    "        \n",
    "        flag_wrong, signals = get_channels_from_raw(raw)\n",
    "        if flag_wrong:\n",
    "            continue\n",
    "        \n",
    "        # make_a_sample_plot_from_array(signals, thisFS)\n",
    "        \n",
    "        # make_a_frequency_plot_from_array(signals, thisFS)\n",
    "        \n",
    "        ### Butter bandpass filter\n",
    "        filtered_signals = []\n",
    "        for i in range(signals.shape[0]):\n",
    "            bandpass_filtered_signal = butter_bandpass_filter(signals[i,:], lowcut, highcut, fs, order=3)\n",
    "            filtered_1_signal = lfilter(notch_1_b, notch_1_a, bandpass_filtered_signal)\n",
    "            filtered_60_signal = lfilter(notch_60_b, notch_60_a, filtered_1_signal)\n",
    "            filtered_signals.append(filtered_60_signal)\n",
    "            \n",
    "        # make_a_filtered_plot_for_comparison(signals, filtered_signals, thisFS)\n",
    "        # plot_signal_in_frequency(signals[0], filtered_signals[0], thisFS)\n",
    "        # # print('Sampling Frequency is: ', thisFS)\n",
    "\n",
    "        # break\n",
    "        ### Resampling\n",
    "        # resampled_signals = []\n",
    "        if thisFS == resampleFS:\n",
    "            resampled_signals = filtered_signals[:]\n",
    "        else:\n",
    "            resampled_signals = resample_data_in_each_channel(filtered_signals, thisFS, resampleFS)\n",
    "            # print(thisFS)\n",
    "            # make_a_resampled_plot_for_comparison(filtered_signals, resampled_signals, thisFS, resampleFS)\n",
    "                \n",
    "        ### Read tse labels\n",
    "        labels = []\n",
    "        if binary_classifier_flag:\n",
    "            tseFile = data_path[:-4] + '.tse_bi'\n",
    "        else:\n",
    "            tseFile = data_path[:-4] + '.tse'\n",
    "        with open(tseFile,'r') as tseReader:\n",
    "            rawText = tseReader.readlines()[2:]\n",
    "            # seizPeriods = []\n",
    "            for item in rawText:\n",
    "                labels.append([int(item.split()[0].split('.')[0]),int(item.split()[1].split('.')[0]),item.split()[2]])\n",
    "\n",
    "        # print(labels)\n",
    "        # break\n",
    "        \n",
    "        ### Slice signal into segments, and assign proper labels\n",
    "        if binary_classifier_flag:\n",
    "            # segments = slice_signals_into_binary_segments(filtered_signals, thisFS, labels, segment_interval, seizure_types, seizure_overlapping_ratio)\n",
    "            segments = slice_signals_into_binary_segments(resampled_signals, resampleFS, labels, segment_interval, seizure_types, seizure_overlapping_ratio)\n",
    "        else:\n",
    "            # segments = slice_signals_into_multiclass_segments(filtered_signals, thisFS, labels, segment_interval, seizure_types, seizure_overlapping_ratio)\n",
    "            segments = slice_signals_into_multiclass_segments(resampled_signals, resampleFS, labels, segment_interval, seizure_types, seizure_overlapping_ratio)\n",
    "        \n",
    "        # segments[:]: list for different seizure labels\n",
    "        # segments[0][:]: list for different annotation lines within the same annotation file (.tse)\n",
    "        # segments[0][0][:]: list for different windows of EEG signals\n",
    "        # segments[0][0][0]: a numpy array of the shape (22, FS*segment_interval)\n",
    "\n",
    "        # if segments[0]:\n",
    "        #     break\n",
    "        \n",
    "        for i in range(len(segments)):\n",
    "            if segments[i] and segments[i][0]:\n",
    "                this_array  = []\n",
    "                this_labels = seizure_types[i]\n",
    "                for j in range(len(segments[i])):\n",
    "                    if not segments[i][j]:\n",
    "                        continue\n",
    "                    for k in range(len(segments[i][j])):\n",
    "                        this_array.append(segments[i][j][k])\n",
    "                \n",
    "                if not os.path.exists(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec')):\n",
    "                    os.mkdir(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec'))\n",
    "                if not os.path.exists(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec', flag_train_val_test)):\n",
    "                    os.mkdir(os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec', flag_train_val_test))\n",
    "                save_folder = os.path.join(save_root, 'segment_interval_'+str(segment_interval)+'_sec', flag_train_val_test, this_labels)\n",
    "                if not os.path.exists(save_folder):\n",
    "                    os.mkdir(save_folder)\n",
    "                save_file = os.path.join(save_folder, patient_session_token+'.npy')\n",
    "                if os.path.isfile(save_file):\n",
    "                    # If the file exists, load the existing data\n",
    "                    existing_data = np.load(save_file, allow_pickle=True)\n",
    "                    # Append the new data to the existing data\n",
    "                    new_data = np.concatenate((existing_data, np.array(this_array)))\n",
    "                    # Save the combined data to the file\n",
    "                    np.save(save_file, new_data)\n",
    "                    print(new_data.shape)\n",
    "                else:\n",
    "                    print(np.array(this_array).shape)\n",
    "                    np.save(save_file, np.array(this_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
